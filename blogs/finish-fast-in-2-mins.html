<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Playwright Tests: Yes, you can finish ALL your playwright tests in 2 to 5 mins!</title>
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>
    <header>
        <h1>Power Tester</h1>
        <nav>
            <a href="../index.html">Home</a>
            <a href="../youtube.html">YouTube</a>
            <a href="../github.html">GitHub</a>
            <a href="../community.html">4TestCommunity</a>
            <a href="../contact.html">Contact</a>
            <a href="../talks.html">Talks</a>
            <a href="../blogs.html">Blogs</a>
            <a href="../books.html">Books</a>
            <a href="../articles.html">Articles</a>
        </nav>
    </header>
    <main>
        <h2>Playwright Tests: Yes, you can finish ALL your playwright tests in 2 to 5 mins!</h2>
        <img src="../images/3k-tests-in-90-seconds.png" alt="3k-tests-in-90-seconds" class="blog-icon" style="width: 55%;">

        <article>
            <h3>Problem Statement</h3>
            <p>Playwright provides a option to distribute tests on multiple machines with its
                <a href="https://playwright.dev/docs/test-sharding" target="_blank">Test Sharding option.</a>
                However there are two main problems with it:
            <ol>
                <h4>Sharding is not aware of how much time each test takes.</h4>
                <li>Even though playwright sharding tries to evenly distributes tests, as explained here in the article about 
                     <a href="https://playwright.dev/docs/test-sharding#balancing-shards" target="_blank">Balancing Shards</a>, in terms of execution time, not all tests take equal time. 
                    <p>
                        And since playwright sharding does not keep a track of time each test takes, this means, there will be some runners that will finish early and some that will finish late.
                        Eventually your total test run time will be bottlenecked due to the slowest running tests.
                    </p>

                    <p>
                        Often the only way team tries to bring down total execution time is by increasing the number of runners, which increases both the cost but also the waste per runner since not all runners are utilized to their full potential.
                    </p>

                </li>
                <img src="../images/uneven-test-run.png" alt="uneven-test-run" class="blog-icon" style="width: 50%;"> 
                <h4>No examples of dynamic scaling of runners</h4>
                <li>Playwright provides a 
                    <a href="https://playwright.dev/docs/test-sharding#github-actions-example" target="_blank">Github Actions Example</a>
                    to show how to define runners for sharding. However, since often the same reusable workflow is used to run all tests, 
                    and test fixes, there are times, when the number of tests touched are very few, but the reusable workflow will still spin up all the runners.
                    This results into a lot of wasted resources and money.
                    <p>
                        NOTE: As mentioned in previous point, since team often tries to increase the runners to bring down execution time, 
                        it means, the waste is very high, if the team does not have a way to scale runners down, when only fixing a few broken tests.
                    </p>
                </li>
                <img src="../images/too-many-runners.png" alt="too-many-runners" class="blog-icon" style="width: 50%;">
            </ol>
            </p>
            <h3>Consequences and results</h3>
            <ol>
                <li>
                    <h4>Consequence: Teams do not run system tests as a part of their pull requests but after its already merged in the main branch.</h4>
                    <p>
                        <b>Result:</b> As a result, tests give <b style="color: red;">false positives</b> and often fail because of new changes and not because there are bugs in
                        the code.
                    </p>
                    <p>
                        <b style="color: red;">This corrodes the trust for tests in the team.</b>
                    </p>
                </li>  
                <li>
                    <h4>Consequence: QAs in the team hence often spend most of their time fixing these "false positives".</h4>
                    <p>
                        <b>Result:</b> As a result, this gives testers <b style="color: red;">"Test maintenance fatigue" and heavily bring down their morale and their ability to have a real impact on the product</b>. 
                    </p>
                    <p>QAs find themselves in this never ending cycle of
                        fixing broken tests, that bring little value to the team and <b style="color: red;"> leave them with little time to actually test the new features or add
                        new
                        tests.</b>
                    </p>
                </li>
                <li>
                    <h4>Consequence: Buying more infrastructure has diminishing results with increased costs.</h4>
                    <p>
                        As we saw earlier, with existing solutions, <b style="color: red;">adding more infrastructure has a diminishing effect on overall test run time and has a direct relation to increased waste (inefficiencies) and costs.</b>
                    </p>
                </li>
            </ol>
            </p>
            <h3>Desired Solution</h3>
            <p>A desired solution would involve creating a dynamic test sharding mechanism that is aware of how much time each test takes and depending on the load of tests and 
                desired total execution time, creates required runners at the run time. 
                <p>
                    This means, if we have a lot of tests to run, then based on the desired total execution time, we can create bundles of tests based on their run time, 
                    and create as many runners as required to cover all tests in the given time. In this model, one runner may run 20 small tests where another may run only 2 big tests.
                    Where each runner, does not surpass the total execution time limit (of say 2 mins)
                </p>
                <img src="../images/dynamic-runners-for-dynamic-load.png" alt="dynamic-runners-for-dynamic-load" class="blog-icon" style="width: 45%;">
                <p>
                    It also means, that if we only touch a few tests and want to finish them in 1 or 2 mins, then we only need to spin up the bare minimum runners
                    to finish all tests in 1 or 2 mins.
                </p>
                <img src="../images/less-runnes-for-less-load.png" alt="less-runnes-for-less-load" class="blog-icon"
                    style="width: 50%;">
            </p>
            <h3>Consequences and results</h3>
            <ol>
                <li>
                    <h4>Consequence: Teams can now run their system tests as a part of their pull requests since now they control how long it should take for their test run to finish.</h4>
                    <p>
                        <b>Result:</b> As a result, developers get a feedback in their PR and can fix the broken tests as a part of their pull requests. This results into tests that gives <b style="color: blue;">true positives</b> once they are merged in the main branch.
                    </p>
                    <p>
                        <b style="color: blue;">This should considerably improve the trust for tests in the team.</b>
                    </p>
                </li>
                <li>
                    <h4>Consequence: Since maintenance is now distributed among all developers, QAs in the team actually have time to do more meaningful activities.</h4>
                    <p>
                        <b>Result:</b> By making everyone responsible for fixing the tests that are breaking due to their own changes, we create a model that scales better with time.</b>
                    </p>
                    <p><b style="color: blue;"> This eliminates the root cause for test maintenance fatigue. Not only developers get timely feedback on their breaking changes but now QAs also find time to test and explore the
                    application, write missing automated tests, and mentor team members by taking workshops and reviewing pull requests</b>
                    </p>
                </li>
                <li>
                    <h4>Consequence: We have a system that scales infra up or down based on the demand and scales well with test scope.</h4>
                    <p>
                        <b>Result:</b> Since our solution is time aware of each test, and dynamically scales infrastructure up or down based on test scope, <b style="color: blue;"> we now have a system, where we pay for what we use, eliminate waste and still keep our test run time within our acceptable time limits.</b>
                    </p>
                </li>
            </ol>
            </p>
            <h3>Solution Design</h3>
            <iframe title="Diagram Viewer" style="width:100%;height:1053px;"
                src="https://viewer.diagrams.net/?tags=%7B%7D&lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=finish-tests-in-x-mins.drawio#Uhttps%3A%2F%2Fraw.githubusercontent.com%2FPramodKumarYadav%2Ftest-art%2Fmain%2Fcicd%2Ffinish-tests-in-x-mins.drawio">
            </iframe>
            <p>Below is a explain of how the above solution works:
                The key components of this design are:
                <h4>STATE.JSON</h4>  
                <ol>
                    <li>This file should contain a map of each test case and the time it took to run on the local machine. </li>
                    <li>The updated tests should run automatically when a developer tries to commit these tests (using pre-commit hooks). </li>
                    <li>File should be automatially created after each test run (using a custom runner) that runs automatically after each run.</li>
                    <li>And by committing this file automatically (using a post-commit hook).</li>
                </ol> 
                <h4>A Custom GitHub action</h4>
                <ul>
                    <li> INPUTS: 
                        <ol>
                            <li> A custom github action that takes users wishes to finish test in (x mins). </li>
                            <li>Takes the command to run </li>
                            <li>Considers the cores of the runner machine to decide workers </li>
                            <li>Considers the cores of the runner machine to decide workers </li>
                        </ol>
                    </li>
                    <li> REFERENCES 
                        <ol>
                            <li>Considers the state.json to undrestand time each test takes to finish</li>
                            <li>Considers the cores of the runner machine to decide the workers that can be safely used for parallel run</li>
                        </ol>
                    </li>
                    <li> OUTPUTS 
                        <ol>
                            <li> A dynamic matrix strategy that gives runner count to run tests. ex: ["1","2", "3"] </li>
                            <li> A test load distribution json document that gives a mapping of tests and their respectie projects to run under
                                each runner </li>
                            <li> The number of workers to use for parallel execution, based on cores of runners to use</li>
                        </ol>
                    </li>
                    <li>CALLER WORKFLOW:
                        <ol>
                            <li> This workflow will use the above variables to run tests efficiently on different runners </li>
                            <li> Create and publish a consolidated html report </li>
                        </ol>
                    </li>
                </ul>     
            </p>
            <h3>Actual Solution</h3>
            <p>
                Here is the reference to the actual solution 
                <a href="https://github.com/PramodKumarYadav/runwright"
                    target="_blank">RunWright
                </a>
                (a custom github action and caller workflow code example), that shows how different pieces come together to make it all
                work).
                
            </p>
            <h3>Reference</h3>
            <p>
                Reference spreadseheet: 
                <a href="https://docs.google.com/spreadsheets/d/18MjzNY7tWU-RtO3osJ9ebYEAR6kHJ7mXUXVFBcFEMAs/edit?usp=sharing"
                    target="_blank">Spreadsheet
                </a> 
            </p>
        </article>
    </main>
    <footer>
        <p>&copy; 2025 Power Tester</p>
    </footer>
</body>

</html>
